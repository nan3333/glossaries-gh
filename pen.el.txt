transvariadicator
    [#pen.el]

    The variadic arg transformer.

    This is an elisp function or a shell
    command to transform the variadic arg into
    final text which is inserted into the
    template.

prompt function
prompting function
f_prompt(Â·)
    Each .prompt file generates a function in
    emacs lisp.

    This generated function I will refer to as
    a prompt function.

nucleus sampling
top p sampling
    Compute the cumulative distribution and
    cut off as soon as the CDF exceeds P.
    
    Example:
        In a broad distribution, it may take
        the top 100 tokens to exceed top_p =
        .9.
        
        In a narrow distribution, we may
        already exceed top_p = .9.
    
    Still avoid sampling egregiously wrong
    tokens, but preserve variety when the
    highest scoring tokens have low
    confidence.

    A method of determining which tokens to
    use that looks at the combined probability
    of groups of tokens.

    See "Top P".

    v +/"Nucleus sampling" "$MYGIT/mullikine/gpt-2/src/sample.py"

    https://arxiv.org/pdf/1904.09751.pdf

Top P
    [#OpenAI API]
    [API setting]
    [GPT-3 prompt setting]
    [GPT hyperparameter]
    [#GPT]

    Floating point value.

    Max value: 1
    Recommended value: 1
    Min value: 0
    
    https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277

    Diversity via nucleus sampling.

    Cuts off tokens below a certain
    probability threshold.

    We generally recommend keeping it set to
    around 1.

    Similar to temperature, this is an API
    setting that controls the diversity of
    tokens considered using nucleus sampling.
    
    A lower setting results in less random
    completions a higher setting is less
    deterministic and repetitive.
    
    Lower settings are generally better for
    classification tasks and higher settings
    for more open-ended responses.

    Controls diversity via nucleus sampling:
    1.5 means half of all likelihood-weighted
    options are considered.

    See "nucleus sampling".

prompt description
prompt description file
prompt description YAML
.prompt
    [#pen.el] 

    A yaml containing the definition of an
    individual prompt.

    Parameters:
    - model
      [string]

    - prompt
      [string]

    - Best of
      [int]

      Recommended value: 1
      Min value: 1
      Max value: 20

    - Frequency Penalty
      [float]

      Default value: 0
      Min value: 0
      Max value: 1

    - top-p
      [float]

      Recommended value: 1
      Max value: 1
      Min value: 0

    - temperature
      [float]

      Recommended value: 0.9 
      Min value: 0
      Max value: 1

    - restart sequence
      [string]

    - start sequence
      [string]

    - stop sequences
      [list of strings]

    - response length
      [int]

      Min value: 64
      Max value: 2048

    - presence penalty
      [float]

      Default value: 0
      Min value: 0
      Max value: 1

chaining
    [#prompt engineering]

    http://gptprompts.wikidot.com/chaining:chaining

    API results into one another: The results
    of one API call can be processed and fed
    back into another API call with a
    different prompt to expand the potential
    capabilities of a workflow.
        
    While OpenAI API has a lot of capabilities
    on its own, it is limited to 2000
    characters per prompt.
    
    This puts a need to be able to determine
    optimal types of prompt selection and
    examples included in the prompt for
    complex queries.
    
    Rather than try to do everything at once,
    subtasks can be delegated to different API
    calls with subtask-optimized prompts.
    
    I don't know if there's a better term for
    this, so I'm tentatively calling this
    behavior chaining and can change this if
    it turns out there's a better name.
    
    Chaining ends up boiling down into a
    couple different tasks.
    
    First, you need to figure out what a
    prompt is actually asking for.
    
    To do this you can decompose the task into
    its component parts and figure out the
    sort of prompts you need to query to
    answer the parts http://gptprompts.wikidot.com/chaining:using-the-fringe.
    
    For each of those sub-parts, you need to
    be able to figure out how to optimize the
    prompt so that it optimizes the answer via
    context stuffing
    http://gptprompts.wikidot.com/context-stuffing.
    
    However, during all of this, you want to
    make sure that you're ensuring that the
    API is in compliance with safety practices
    for your task
    http://gptprompts.wikidot.com/safety.
    
context-stuffing
    [#prompt engineering]

    With only 2048 tokens, you need to make
    use of your real estate by providing
    instructions and making implicit
    information explicit.

benchmarking
    [#prompt engineering]

    A single prompt getting a good response
    once might be enough for your use case,
    but generally we want to know how well
    different solutions hold up on a variety
    of use cases.
    
    Also important to understand where prompt
    designs outperform others.

explicit bias
    [#ai safety]

    Where output is clearly toxic (cursing,
    slander).

implicit bias
    [#ai safety]

    Where the policy from the output changes
    based on context (e.g. opposition to
    programs that help certain groups,
    probabilities about roles).

openai-ft
    [cli command]

    Run a fine-tuning job using OpenAI
    finetuning API.

    py -36 i openai-finetune

openai-ft-classification
    [cli command]

    Run a classification fine-tuning job using
    OpenAI finetuning API

    py -36 i openai-finetune

openai-ft-events
    [cli command]

    List the events for a batch-mode
    fine-tuning run.

    py -36 i openai-finetune

openai-ft-report
    [cli command]

    List the events for a batch-mode
    fine-tuning run.

    py -36 i openai-finetune

PEN_RESULT
    http://github.com/semiosis/prompts/blob/master/prompts/imagine-a-website-from-a-url-1.prompt

    The text that begins at
    PEN_COLLECT_FROM_POS (before
    PEN_INJECT_GEN_START) but has even more
    text.

PEN_COLLECT_FROM_POS
    http://github.com/semiosis/prompts/blob/master/prompts/imagine-a-website-from-a-url-1.prompt

    Just before the URL.

PEN_END_POS
    http://github.com/semiosis/prompts/blob/master/prompts/imagine-a-website-from-a-url-1.prompt

    The length of the prompt including PEN_INJECT_GEN_START.

    The end of the prompt (including the
    PEN_INJECT_GEN_START, which is the last
    part of the prompt).

PEN_INJECT_GEN_START
    http://github.com/semiosis/prompts/blob/master/prompts/imagine-a-website-from-a-url-1.prompt

    The text that begins just after
    PEN_COLLECT_FROM_POS. There is no recorded
    byte position. It was the previous
    generation.

PEN_PROMPT
    http://github.com/semiosis/prompts/blob/master/prompts/imagine-a-website-from-a-url-1.prompt

    The entire prompt including the
    PEN_INJECT_GEN_START string.

garble
    Reproduce (a message, sound, or
    transmission) in a confused and distorted
    way.
    "the connection was awful and kept
    garbling his voice"

garbify
    [#pen]

    ascify then remove whitespace. This is to
    make a block of text ready for
    classification but to avoid
    breaking/confusing prompts with their
    structure.

    Good for screenshots.